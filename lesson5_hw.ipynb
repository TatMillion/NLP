{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62146729",
   "metadata": {},
   "source": [
    "# Введение в обработку естественного языка\n",
    "## Урок 5. Part-of-Speech разметка, NER, извлечение отношений\n",
    "\n",
    "Задание 1. Написать теггер на данных с русским языком\n",
    "проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
    "написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "сравнить все реализованные методы, сделать выводы  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f2799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "import pyconll\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443cb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-a.conllu')\n",
    "full_train_b = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-b.conllu')\n",
    "full_train_c = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-c.conllu')\n",
    "\n",
    "# Общая обучающая выборка\n",
    "full_train.extend([*full_train_b, *full_train_c])\n",
    "\n",
    "full_test = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in full_test[:1]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "display(unigram_tagger.tag(fdata_sent_test[50]), unigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5002ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "display(bigram_tagger.tag(fdata_sent_test[50]), bigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "display(trigram_tagger.tag(fdata_sent_test[50]), trigram_tagger.evaluate(fdata_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "# В качестве бэкофф опции будем использовать тэг существительного\n",
    "backoff = DefaultTagger('NOUN') \n",
    "tag = backoff_tagger(fdata_train,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        if (tok[0] is None) or (tok[1] is None):\n",
    "            continue\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        if (tok[0] is None) or (tok[1] is None):\n",
    "            continue\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d908e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(2, 15), analyzer='char', n_features=65536)\n",
    "tvectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word')\n",
    "cvectorizer = CountVectorizer(ngram_range=(2, 13), analyzer='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff523e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xh_train = hvectorizer.fit_transform(train_tok)\n",
    "Xh_test = hvectorizer.transform(test_tok)\n",
    "\n",
    "Xt_train = tvectorizer.fit_transform(train_tok)\n",
    "Xt_test = tvectorizer.transform(test_tok)\n",
    "\n",
    "Xc_train = cvectorizer.fit_transform(train_tok)\n",
    "Xc_test = cvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd64296",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=42, max_iter=500)\n",
    "lr.fit(Xh_train, train_enc_labels)\n",
    "pred = lr.predict(Xh_test)\n",
    "print(f'Accuracy на основе HashingVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=42, max_iter=500)\n",
    "lr.fit(Xt_train, train_enc_labels)\n",
    "pred = lr.predict(Xt_test)\n",
    "print(f'Accuracy на основе TfidfVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=42, max_iter=500)\n",
    "lr.fit(Xc_train, train_enc_labels)\n",
    "pred = lr.predict(Xc_test)\n",
    "print(f'Accuracy на основе CountVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40be19a",
   "metadata": {},
   "source": [
    "Как видим наилучшая точность получилась среди векторайзеров получилась для модели CountVectorizer на основе букв\n",
    "\n",
    "Задание 2. Проверить, насколько хорошо работает NER\n",
    "Данные брать из Index of /pub/named_entities\n",
    "проверить NER из nltk/spacy/deeppavlov.\n",
    "написать свой NER, попробовать разные подходы.\n",
    "передаём в сетку токен и его соседей.\n",
    "передаём в сетку только токен.\n",
    "свой вариант.\n",
    "сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install natasha corus\n",
    "!pip -q install spacy\n",
    "!python -m spacy download ru_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1a628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_ru')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "\n",
    "from corus import load_ne5\n",
    "from razdel import tokenize\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99cdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Collection5/'\n",
    "records = load_ne5(dir)\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceef708",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_ne5(dir)\n",
    "for ix, rec in enumerate(records):\n",
    "  print(rec.text)\n",
    "  print('\\nИменованные сущности:')\n",
    "  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(rec.text), lang='rus')):\n",
    "    if hasattr(chunk, 'label'):\n",
    "      print(f'{chunk} - {chunk.label()}')\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for rec in records:\n",
    "    words = []\n",
    "    labels = []\n",
    "    idx_ent = -1\n",
    "    len_ents = len(rec.spans)\n",
    "    rec_entities = sorted(rec.spans, key=lambda v: v.start)\n",
    "    ent = None\n",
    "    is_start = None\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        if len_ents == 0:\n",
    "            words.append(token.text)\n",
    "            labels.append(type_ent)\n",
    "            continue\n",
    "\n",
    "        if (idx_ent == -1) or (idx_ent + 1 < len_ents and token.start > ent.stop):\n",
    "            idx_ent += 1\n",
    "            ent = rec_entities[idx_ent]\n",
    "            is_start = True\n",
    "\n",
    "        if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = 'B-' + ent.type if is_start else 'I-' + ent.type\n",
    "                is_start = False\n",
    "        words.append(token.text)\n",
    "        labels.append(type_ent)\n",
    "    \n",
    "    docs.append([words, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1224e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0][0])\n",
    "print(docs[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_coeff = 0.75\n",
    "\n",
    "with open('c5.bio', 'w') as w:\n",
    "    with open('c5_train.bio', 'w') as w1:\n",
    "        with open('c5_valid.bio', 'w') as w2:\n",
    "            for irec, rec in enumerate(docs):\n",
    "                for line in map(lambda vl: '\\t'.join(vl) + '\\n', zip(*rec)):\n",
    "                    w.write(line)\n",
    "                    if irec < len(docs) * training_coeff:\n",
    "                        w1.write(line)\n",
    "                    else:\n",
    "                        w2.write(line)\n",
    "                w.write('\\n')\n",
    "                if irec < len(docs) * training_coeff:\n",
    "                    w1.write('\\n')\n",
    "                else:\n",
    "                    w2.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ffb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init config base_config.cfg -F -p  ner -l ru\n",
    "!python -m spacy init fill-config base_config.cfg config.cfg\n",
    "# !python -m spacy convert c5.bio . -t json -c ner\n",
    "!python -m spacy convert c5_train.bio . -c ner\n",
    "!python -m spacy convert c5_valid.bio . -c ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70084e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --output ./output --paths.train c5_train.spacy --paths.dev c5_valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d69755",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy evaluate output/model-last c5_valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deeppavlov\n",
    "!python -m deeppavlov install squad_bert\n",
    "!python -m deeppavlov install ner_ontonotes\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ac8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov import configs, build_model\n",
    "\n",
    "ner_model = build_model(configs.ner.ner_few_shot_ru, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.utils import parse_config\n",
    "config_dict = parse_config(configs.ner.ner_few_shot_ru)\n",
    "print(config_dict['dataset_reader']['data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ner_few_shot_data/all.txt', 'w') as w:\n",
    "    with open('ner_few_shot_data/train.txt', 'w') as w1:\n",
    "        with open('ner_few_shot_data/valid.txt', 'w') as w2:\n",
    "          with open('ner_few_shot_data/test.txt', 'w') as w3:\n",
    "            for irec, rec in enumerate(docs):\n",
    "                for line in map(lambda vl: '\\t'.join(vl) + '\\n', zip(*rec)):\n",
    "                    w.write(line)\n",
    "                    if irec < 40:\n",
    "                        w1.write(line)\n",
    "                    elif irec < 45:\n",
    "                        w2.write(line)\n",
    "                    elif irec < 50:\n",
    "                        w3.write(line)\n",
    "                w.write('\\n')\n",
    "                if irec < 40:\n",
    "                    w1.write(line)\n",
    "                elif irec < 45:\n",
    "                    w2.write(line)\n",
    "                elif irec < 50:\n",
    "                    w3.write(line)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20232cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov train ner_few_shot_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov import train_model\n",
    "ner_model = train_model(configs.ner.ner_few_shot_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a01d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = [docs[i][1] for i in range(45, 50)]\n",
    "y_true = [item for sublist in y_t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = []\n",
    "for i in range(45, 50):\n",
    "  y_p.append(ner_model(docs[i][0])[1])\n",
    "\n",
    "y_pred = [item for sublist in y_p for item in sublist]\n",
    "y_pred = [item for sublist in y_pred for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff82c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_true)\n",
    "y_true[np.char.endswith(y_true, 'MEDIA')]='MEDIA'\n",
    "y_true[np.char.endswith(y_true, 'GEOPOLIT')]='GEOPOLIT'\n",
    "y_true[np.char.endswith(y_true, 'LOC')]='LOC'\n",
    "y_true[np.char.endswith(y_true, 'PER')]='PER'\n",
    "y_true[np.char.endswith(y_true, 'ORG')]='ORG'\n",
    "y_true[np.char.endswith(y_true, 'OUT')]='OUT'\n",
    "np.unique(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d9c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred)\n",
    "y_pred[np.char.endswith(y_pred, 'MEDIA')]='MEDIA'\n",
    "y_pred[np.char.endswith(y_pred, 'GEOPOLIT')]='GEOPOLIT'\n",
    "y_pred[np.char.endswith(y_pred, 'LOC')]='LOC'\n",
    "y_pred[np.char.endswith(y_pred, 'PER')]='PER'\n",
    "y_pred[np.char.endswith(y_pred, 'ORG')]='ORG'\n",
    "y_pred[np.char.endswith(y_pred, 'OUT')]='OUT'\n",
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afbf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn import model_selection, preprocessing, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9936ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Collection5/'\n",
    "records = load_ne5(dir)\n",
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892613bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])\n",
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8394502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "# без соседних токенов \n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    # ngrams=(1, 3),\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "\n",
    "# с соседними токенами\n",
    "vectorize_layer_n13 = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    ngrams=(1, 3),\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "vectorize_layer_n4 = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    ngrams=4,\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "modeln = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    Conv1D(embedding_dim, 3),\n",
    "    Conv1D(embedding_dim, 2),\n",
    "    GRU(350),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(6, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7366e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeln.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81220769",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeln.fit(train_data, validation_data=valid_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_n13.adapt(text_data)\n",
    "\n",
    "modeln13 = Sequential([\n",
    "    vectorize_layer_n13,\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    Conv1D(embedding_dim, 3),\n",
    "    Conv1D(embedding_dim, 2),\n",
    "    GRU(350),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "modeln13.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modeln13.fit(train_data, validation_data=valid_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_n4.adapt(text_data)\n",
    "\n",
    "modeln4 = Sequential([\n",
    "    vectorize_layer_n4,\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    Conv1D(embedding_dim, 3),\n",
    "    Conv1D(embedding_dim, 2),\n",
    "    GRU(350),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "modeln4.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modeln4.fit(train_data, validation_data=valid_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_predict_n = modeln.predict(valid_data)\n",
    "labels_predict_n13 = modeln13.predict(valid_data)\n",
    "labels_predict_n4 = modeln4.predict(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_preds_n = np.argmax(tf.nn.softmax(labels_predict_n), axis=1)\n",
    "class_preds_n13 = np.argmax(tf.nn.softmax(labels_predict_n13), axis=1)\n",
    "class_preds_n4 = np.argmax(tf.nn.softmax(labels_predict_n4), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa197b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = encoder.inverse_transform(valid_y)\n",
    "class_preds_n = encoder.inverse_transform(class_preds_n)\n",
    "class_preds_n13 = encoder.inverse_transform(class_preds_n13)\n",
    "class_preds_n4 = encoder.inverse_transform(class_preds_n4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(valid_y, class_preds_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea99334",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(valid_y, class_preds_n13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(valid_y, class_preds_n4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470f848",
   "metadata": {},
   "source": [
    "модель построенная с помощью библиотеки spacy показала самые лучшие результаты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
