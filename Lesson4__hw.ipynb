{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0287326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "  Downloading natasha-1.5.0-py3-none-any.whl (34.4 MB)\n",
      "     ---------------------------------------- 34.4/34.4 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting razdel>=0.5.0\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Collecting slovnet>=0.6.0\n",
      "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
      "     -------------------------------------- 46.7/46.7 kB 291.4 kB/s eta 0:00:00\n",
      "Collecting ipymarkup>=0.8.0\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pymorphy2 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from natasha) (0.9.1)\n",
      "Collecting yargy>=0.14.0\n",
      "  Downloading yargy-0.15.1-py3-none-any.whl (33 kB)\n",
      "Collecting navec>=0.9.0\n",
      "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Collecting funcy\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (67.1.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (1.3.5)\n",
      "Requirement already satisfied: gensim in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (1.21.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: future in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (0.18.3)\n",
      "Requirement already satisfied: numexpr in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post4.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting intervaltree>=3\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2022.7)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Collecting sortedcontainers<3.0,>=2.0\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn, intervaltree\n",
      "  Building wheel for pyLDAvis (pyproject.toml): started\n",
      "  Building wheel for pyLDAvis (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136904 sha256=42461a07568ae6b1bb654e09e09a4fd10ba1bd858c5b97f9dcf520c1383e1aeb\n",
      "  Stored in directory: c:\\users\\tato\\appdata\\local\\pip\\cache\\wheels\\c9\\21\\f6\\17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post4-py3-none-any.whl size=2982 sha256=87161ac87d3f1219cb415fbcb3925cab3098f38dcb31a7e493e9ecedc48c8872\n",
      "  Stored in directory: c:\\users\\tato\\appdata\\local\\pip\\cache\\wheels\\89\\65\\a1\\b406f8c3f0a3c5fe55c18adf9f8e06b38a30f127b086b867d4\n",
      "  Building wheel for intervaltree (setup.py): started\n",
      "  Building wheel for intervaltree (setup.py): finished with status 'done'\n",
      "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26128 sha256=e238f1eeb3288f4ec51818c9fb12583a42e6e96d31151b6ba7d4b1c4895b1284\n",
      "  Stored in directory: c:\\users\\tato\\appdata\\local\\pip\\cache\\wheels\\16\\85\\bd\\1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
      "Successfully built pyLDAvis sklearn intervaltree\n",
      "Installing collected packages: sortedcontainers, sklearn, razdel, funcy, navec, intervaltree, yargy, slovnet, ipymarkup, pyLDAvis, natasha\n",
      "Successfully installed funcy-2.0 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.5.0 navec-0.10.0 pyLDAvis-3.3.1 razdel-0.5.0 sklearn-0.0.post4 slovnet-0.6.0 sortedcontainers-2.4.0 yargy-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install natasha pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "from gensim.models import *\n",
    "from gensim import corpora\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/rospotrebnadzor.csv', index_col=0)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "stopwords_list = stopwords.words('russian')\n",
    "stopwords_list.append('это')\n",
    "stopwords_list.append('здравствуйте')\n",
    "stopwords_list.append('добрый')\n",
    "stopwords_list.append('день')\n",
    "stopwords_list.append('год')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ae4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_regex = re.compile('\\w+')\n",
    "\n",
    "def find_words(text, regex = words_regex):\n",
    "    tokens =  regex.findall(text.lower())\n",
    "    return [w for w in tokens if w.isalpha() and len(w) >= 3]\n",
    "\n",
    "def lemmatize(words, lemmer = morph, stopwords = stopwords_list):\n",
    "    lemmas = [lemmer.parse(w)[0].normal_form for w in words]\n",
    "    return [w for w in lemmas if not w in stopwords and w.isalpha()]\n",
    "\n",
    "def preprocess(text):\n",
    "    return (lemmatize(find_words(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_questions'] = df['questions'].apply(preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(df['preprocessed_questions'])\n",
    "\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.85, keep_n=None)\n",
    "dictionary.save('rpn.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ca983",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(question) for question in df['preprocessed_questions']]\n",
    "corpora.MmCorpus.serialize('rpn.model', corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=15, \n",
    "                        chunksize=100, update_every=1, passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Персплексия: ', np.exp(lda.log_perplexity(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84715ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda, texts=df['preprocessed_questions'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Средняя когерентность: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_list = [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "coherences = []\n",
    "\n",
    "for num in topics_list:\n",
    "    lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=num, \n",
    "                            chunksize=100, update_every=1, passes=2)\n",
    "    coherences.append(CoherenceModel(model=lda, \n",
    "                                     texts=df['preprocessed_questions'], \n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence='c_v').get_coherence())\n",
    "\n",
    "plt.plot(topics_list, coherences)\n",
    "plt.xlabel(\"Число тем\")\n",
    "plt.ylabel(\"Средняя когерентность\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=10, \n",
    "                        chunksize=100, update_every=1, passes=2)\n",
    "\n",
    "lda.show_topics(num_topics=10, num_words=10, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dab218",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "vis_data = gensimvis.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b45a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
