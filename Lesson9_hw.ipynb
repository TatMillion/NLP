{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49dcd0b6",
   "metadata": {},
   "source": [
    "# Урок 9. Языковое моделирование\n",
    "# Задание\n",
    "Разобраться с моделью генерации текста, собрать самим или взять датасет с вебинара и обучить генератор текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da2bc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tato\\AppData\\Local\\Temp\\ipykernel_17388\\1321173675.py:2: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55fbdb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 286984 characters\n"
     ]
    }
   ],
   "source": [
    "text = open('evgenyi_onegin.txt','rb').read().decode(encoding = 'utf-8')\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8e2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e71c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0238a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0f0b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "А\n",
      "л\n",
      "е\n",
      "к\n",
      "с\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4438fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n",
      "'      Роман в стихах\\n\\n                        Не мысля гордый свет забавить,\\n                        '\n",
      "'Вниманье дружбы возлюбя,\\n                        Хотел бы я тебе представить\\n                        '\n",
      "'Залог достойнее тебя,\\n                        Достойнее души прекрасной,\\n                        Свят'\n",
      "'ой исполненной мечты,\\n                        Поэзии живой и ясной,\\n                        Высоких д'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder = True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "507a892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698282b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
      "Target data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf507add",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8b47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d8fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNgenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(RNNgenerator, self).__init__()\n",
    "        \n",
    "        self.emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "                                 \n",
    "        self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences = True,\n",
    "                            stateful = False,\n",
    "                            recurrent_initializer = 'glorot_uniform')\n",
    "\n",
    "        self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences = True,\n",
    "                            stateful = False,\n",
    "                            recurrent_initializer = 'glorot_uniform')\n",
    "        self.gru3 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences = True,\n",
    "                            stateful = False,\n",
    "                            recurrent_initializer = 'glorot_uniform')\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        emb_x = self.emb(x)\n",
    "        x = self.gru1(emb_x)\n",
    "        x = self.gru2(x)\n",
    "        x = self.gru3(x)\n",
    "\n",
    "        x = self.dense(x)\n",
    "        return x \n",
    "\n",
    "model = RNNgenerator(vocab_size,\n",
    "                     embedding_dim,\n",
    "                     rnn_units = rnn_units\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "933d64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ddcd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = loss, metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88c164ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'RNN_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath = checkpoint_prefix,\n",
    "                                save_freq = 5,\n",
    "                                save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89500f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119da6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNgenerator(vocab_size, embedding_dim, rnn_units)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad012745",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generate = 500\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f78145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0921e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = generate_text(model, start_string = u\"И вот идет уже \")\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39babcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNgenerator(vocab_size, embedding_dim, rnn_units) \n",
    "model.load_weights('RNN_checkpoints/ckpt_200')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = generate_text(model, start_string = u\"И вот идет уже \")\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNgenerator_1(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.gru3 = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, states=None, return_state=False):\n",
    "\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    if states is None:\n",
    "      states = self.gru1.get_initial_state(x)\n",
    "\n",
    "    x, states = self.gru1(x, initial_state=states)\n",
    "    x, states = self.gru2(x, initial_state=states)\n",
    "    x, states = self.gru3(x, initial_state=states)\n",
    "\n",
    "    x = self.dense(x)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "heckpoint_dir = 'RNN_checkpoints'\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath = checkpoint_prefix,\n",
    "                                save_freq = 1,\n",
    "                                save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = RNNgenerator_1(\n",
    "                 vocab_size = vocab_size,\n",
    "                 embedding_dim = embedding_dim,\n",
    "                 rnn_units = rnn_units,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828563c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer = 'adam', loss = loss, metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.load_weights('./RNN_checkpoints/ckpt_200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ddbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "\n",
    "def generate_text_1(model, start_string, states, temperature):\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions, states = model(input_eval, states=states, return_state=True)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a9485",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generate = 500\n",
    "temperature = 0.6\n",
    "states = None\n",
    "next_char = u\"И вот идет уже \"\n",
    "text_generated = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75eee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = generate_text_1(model_1, next_char, states, temperature)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b34520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNgenerator_2(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.gru3 = tf.keras.layers.GRU(rnn_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, states_1=None, states_2=None, states_3=None, return_state=False):\n",
    "\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    if states is None:\n",
    "      states_1 = self.gru1.get_initial_state(x)\n",
    "      states_2 = self.gru1.get_initial_state(x)\n",
    "      states_3 = self.gru1.get_initial_state(x)\n",
    "\n",
    "    x, states_1 = self.gru1(x, initial_state=states_1)\n",
    "    x, states_2 = self.gru2(x, initial_state=states_2)\n",
    "    x, states_3 = self.gru3(x, initial_state=states_3)\n",
    "\n",
    "    x = self.dense(x)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states_1, states_2, states_3\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543481d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'RNN_2_checkpoints'\n",
    "checkpoint_dir = 'RNN_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                filepath = checkpoint_prefix,\n",
    "                                save_freq = 1,\n",
    "                                save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = RNNgenerator_2(\n",
    "                 vocab_size = vocab_size,\n",
    "                 embedding_dim = embedding_dim,\n",
    "                 rnn_units = rnn_units,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer = 'adam', loss = loss, metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d80ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.load_weights('./RNN_checkpoints/ckpt_200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_2(model, start_string, states_1, states_2, states_3, temperature):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions, states_1,states_2, states_3 = model(input_eval, states_1=states_1, states_2=states_2, states_3=states_3, return_state=True)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eded73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generate = 500\n",
    "temperature = .5\n",
    "states_1 = None\n",
    "states_2 = None\n",
    "states_3 = None\n",
    "next_char = u\"И вот идет уже \"\n",
    "text_generated = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa12c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = generate_text_2(model_2, next_char, states_1, states_2, states_3, temperature)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d84d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
