{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade3d56a",
   "metadata": {},
   "source": [
    "# Введение в обработку естественного языка\n",
    "## Урок 14. Transfer learning\n",
    "Задание 14\n",
    "Задание взять данные из https://www.kaggle.com/datasets/mrapplexz/bashim-quotes обучить модель GPT для генерации своих цитат\n",
    "взять новостные данные из https://github.com/natasha/corus load_lenta2 нам понадобиться сам текст и заголовок обучить модель T5/ или GPT для генерации заголовков для статей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7540128",
   "metadata": {},
   "source": [
    "**Генерация цитат**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9981f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: requests in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata->transformers) (3.12.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (4.11.4)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: xxhash in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (2023.1.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (4.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata->datasets) (3.12.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tato\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "pd.set_option('max_colwidth', 400)\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a762227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2004-08-30 11:24:00+00:00</td>\n",
       "      <td>22010.0</td>\n",
       "      <td>&lt;Ares&gt; ppdv, все юниксы очень дружелюбны.. они просто очень разборчивы в друзьях ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2004-08-30 11:25:00+00:00</td>\n",
       "      <td>25105.0</td>\n",
       "      <td>&lt;томатик_рад&gt; а ты не чувствуешь красоту мира?\\n&lt;fox&gt; честно говоря, я сейчас чувствую только отсутствие http.\\n&lt;томатик_рад&gt; не туда смотришь, глянь вокруг!\\n&lt;fox&gt; как я гляну, если http не работает? :/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2004-08-30 11:27:00+00:00</td>\n",
       "      <td>7192.0</td>\n",
       "      <td>&lt;Дор&gt; \"мышка, почему у тебя такие большие глаза?\" УЙДИ!!! я ХАРАКИРИ делаю!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2004-08-30 11:28:00+00:00</td>\n",
       "      <td>29169.0</td>\n",
       "      <td>&lt;PPDV[os2]&gt; \"Мальчики, вы что больные, бегать в палату к девочкам?! - Если б мы были больные - мы б бегали к другим мальчикам\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2004-08-30 11:26:00+00:00</td>\n",
       "      <td>7140.0</td>\n",
       "      <td>&lt;Ohtori_Akio&gt; мы - как разработчики - живём с субейзом под одбц. \\n&lt;Ohtori_Akio&gt; лучше бы мы жили в пещере с гоблинами.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                      date   rating  \\\n",
       "0   1 2004-08-30 11:24:00+00:00  22010.0   \n",
       "1   2 2004-08-30 11:25:00+00:00  25105.0   \n",
       "2   3 2004-08-30 11:27:00+00:00   7192.0   \n",
       "3   4 2004-08-30 11:28:00+00:00  29169.0   \n",
       "4   5 2004-08-30 11:26:00+00:00   7140.0   \n",
       "\n",
       "                                                                                                                                                                                                          text  \n",
       "0                                                                                                                          <Ares> ppdv, все юниксы очень дружелюбны.. они просто очень разборчивы в друзьях ;)  \n",
       "1  <томатик_рад> а ты не чувствуешь красоту мира?\\n<fox> честно говоря, я сейчас чувствую только отсутствие http.\\n<томатик_рад> не туда смотришь, глянь вокруг!\\n<fox> как я гляну, если http не работает? :/  \n",
       "2                                                                                                                             <Дор> \"мышка, почему у тебя такие большие глаза?\" УЙДИ!!! я ХАРАКИРИ делаю!!!!!!  \n",
       "3                                                                               <PPDV[os2]> \"Мальчики, вы что больные, бегать в палату к девочкам?! - Если б мы были больные - мы б бегали к другим мальчикам\"  \n",
       "4                                                                                      <Ohtori_Akio> мы - как разработчики - живём с субейзом под одбц. \\n<Ohtori_Akio> лучше бы мы жили в пещере с гоблинами.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('dataset.jsonl', lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c5cc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81497, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d99811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция создания текстового файла с предобработкой\n",
    "# Уберем никнеймы из цитат\n",
    "\n",
    "def build_text_files(data_json, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    for texts in data_json:\n",
    "        summary = texts.strip()\n",
    "        summary = re.sub(r'<.*?>', '', summary)\n",
    "        data += summary + ' '\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee1b19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.3)\n",
    "\n",
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d9683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 57047\n",
      "Test dataset length: 24450\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset length: ' + str(len(train)))\n",
    "print('Test dataset length: ' + str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "#sberbank-ai/rugpt3large_based_on_gpt2\n",
    "#sberbank-ai/rugpt3medium_based_on_gpt2\n",
    "#sberbank-ai/rugpt3small_based_on_gpt2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/rugpt3medium_based_on_gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('sberbank-ai/rugpt3medium_based_on_gpt2')\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fe538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(train_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2-chief', \n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10, \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4, \n",
    "    eval_steps=100, # Number of update steps between two evaluations.\n",
    "    save_steps=400, # after # steps model is saved\n",
    "    warmup_steps=500, # number of warmup steps for learning rate scheduler\n",
    "    optim = 'adamw_torch',\n",
    "    report_to='all'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c316b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained('gpt_quotes')\n",
    "model.save_pretrained('model_gpt_quotes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be701a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Colab Notebooks/NLP/gpt_quotes')\n",
    "model_new = AutoModelForCausalLM.from_pretrained('/content/drive/MyDrive/Colab Notebooks/NLP/model_gpt_quotes').to('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ff239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quotes(prefix):\n",
    "    tokens = tokenizer(prefix, return_tensors='pt').to('cuda')\n",
    "    size = tokens['input_ids'].shape[1]\n",
    "    output = model_new.generate(\n",
    "        **tokens, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        max_length=size + 50, \n",
    "        repetition_penalty=5., \n",
    "        # temperature=5,\n",
    "        num_beams=80,\n",
    "        # early_stopping=True,\n",
    "        no_repeat_ngram_size=1)\n",
    "\n",
    "    decoded = tokenizer.decode(output[0])\n",
    "    result = decoded[len(prefix):]\n",
    "    print(f'{prefix} {result}\\n *******\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_quotes('Эээ')\n",
    "generate_quotes('Ну что ж')\n",
    "generate_quotes('Сегодня утром')\n",
    "generate_quotes('Я хочу сказать, что')\n",
    "generate_quotes('Кручу верчу')\n",
    "generate_quotes('Не надо изысканности')\n",
    "generate_quotes('О!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caa1b9",
   "metadata": {},
   "source": [
    " Чем больше num_beams, тем лучше генерируется текст"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e030f57",
   "metadata": {},
   "source": [
    "**Генерация заголовков статей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d80722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corus import load_lenta\n",
    "\n",
    "path = 'lenta-ru-news.csv.gz'\n",
    "records = load_lenta(path)\n",
    "next(records)\n",
    "\n",
    "LentaRecord(\n",
    "    url='https://lenta.ru/news/2018/12/14/cancer/',\n",
    "    title='Названы регионы России с\\xa0самой высокой смертностью от\\xa0рака',\n",
    "    text='Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована наиболее высокая смертность от рака, сооб...',\n",
    "    topic='Россия',\n",
    "    tags='Общество'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lenta = pd.read_csv('lenta-ru-news.csv')\n",
    "data_lenta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lenta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lenta.dropna(subset=['text'], inplace=True)\n",
    "data_lenta.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_lenta[:10000], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2291a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset length: ' + str(len(train)))\n",
    "print('Test dataset length: ' + str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sum = int(np.mean([len(data_lenta['text'][txt]) for txt in range(len(data_lenta))])/2.5)\n",
    "max_len_tl = int(np.mean([len(data_lenta['title'][txt]) for txt in range(len(data_lenta))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df65ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(train)\n",
    "dataset_test = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac36bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'IlyaGusev/rut5_base_sum_gazeta'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091127a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenized_input = tokenizer(batch['text'], batch['title'], padding='max_length', truncation=True, max_length=max_len_sum)\n",
    "    # tokenized_label = tokenizer(batch['title'], padding='max_length', truncation=True, max_length=max_len_tl)\n",
    "\n",
    "    # tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "\n",
    "    return tokenized_input\n",
    "\n",
    "\n",
    "# Сформируем токены и удалим лишние столбцы\n",
    "\n",
    "dataset_train = dataset_train.map(tokenize, batched=True, batch_size=8)\n",
    "dataset_test = dataset_test.map(tokenize, batched=True, batch_size=8)\n",
    "\n",
    "dataset_train = dataset_train.remove_columns(['index', 'url', 'title', 'text', 'topic', 'tags', 'date', '__index_level_0__'])\n",
    "dataset_test = dataset_test.remove_columns(['index', 'url', 'title', 'text', 'topic', 'tags', 'date', '__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.save_to_disk('./')\n",
    "dataset_test.save_to_disk('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a279d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'lenta/output'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_accumulation_steps=1, # Number of eval steps to keep in GPU (the higher, the mor vRAM used)\n",
    "    prediction_loss_only=True, # If I need co compute only loss and not other metrics, setting this to true will use less RAM\n",
    "    learning_rate=0.001,\n",
    "    evaluation_strategy='steps', # Run evaluation every eval_steps,\n",
    "    # save_strategy='epoch',\n",
    "    save_steps=500, # How often to save a checkpoint\n",
    "    save_total_limit=1, # Number of maximum checkpoints to save\n",
    "    remove_unused_columns=True, # Removes useless columns from the dataset\n",
    "    # run_name='run_gazeta', # Wandb run name\n",
    "    logging_steps=500, # How often to log loss to wandb\n",
    "    eval_steps=500, # How often to run evaluation on the val_set\n",
    "    logging_first_step=False, # Whether to log also the very first training step to wandb\n",
    "    load_best_model_at_end=True, # Whether to load the best model found at each evaluation.\n",
    "    metric_for_best_model='loss', # Use loss to evaluate best model.\n",
    "    greater_is_better=False, # Best model is the one with the lowest loss, not highest.\n",
    "    report_to='all',\n",
    "    optim = 'adamw_torch',\n",
    "    max_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c05f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf293d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3863236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_generate(text_index):\n",
    "\n",
    "    input_text = test['text'][text_index]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokenized_text = tokenizer(input_text, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "        source_ids = tokenized_text['input_ids'].to(device, dtype = torch.long)\n",
    "        source_mask = tokenized_text['attention_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=source_ids,\n",
    "            attention_mask=source_mask, \n",
    "            max_length=20,\n",
    "            num_beams=10,\n",
    "            # temperature = 1.3,\n",
    "            repetition_penalty=1., \n",
    "            length_penalty=2, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=1\n",
    "        )\n",
    "\n",
    "        pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        print(f\"PREDICTION: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INX = 7471\n",
    "print(f\"TEXT: {test['text'][INX]}\")\n",
    "print(f\"TITLE: {test['title'][INX]}\")\n",
    "title_generate(INX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "INX = 1869\n",
    "print(f\"TEXT: {test['text'][INX]}\")\n",
    "print(f\"TITLE: {test['title'][INX]}\")\n",
    "title_generate(INX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "INX = 8139\n",
    "print(f\"TEXT: {test['text'][INX]}\")\n",
    "print(f\"TITLE: {test['title'][INX]}\")\n",
    "title_generate(INX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d08989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
